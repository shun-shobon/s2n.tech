---
title: Node.js Stream API完全ガイド - 大容量データを効率的に処理しよう
tags:
  - Node.js
  - Stream
  - Backend
  - Performance
publishedAt: "2023-07-05"
updatedAt: "2024-02-18"
---

## はじめに

Node.jsのStream APIは、大容量のデータを効率的に処理するための強力な機能です。
メモリ使用量を抑えながら、リアルタイムでデータの読み書きや変換ができます。
ファイル処理、HTTP通信、データ変換など、様々な場面で活用できます。
今回は、Stream APIの基本から実践的な使い方まで詳しく解説していきます。

## Streamの基本概念

### Streamの種類

Node.jsには4つの基本的なStreamタイプがあります：

```javascript
const { Readable, Writable, Transform, Duplex } = require('stream');

// 1. Readable Stream - データを読み取る
// 2. Writable Stream - データを書き込む  
// 3. Transform Stream - データを変換する
// 4. Duplex Stream - 読み書き両方ができる
```

### Streamの利点

```javascript
// ❌ 従来の方法：全体をメモリに読み込む
const fs = require('fs');

// 大きなファイル（例：1GB）を一度にメモリに読み込む
fs.readFile('huge-file.txt', (err, data) => {
  if (err) throw err;
  console.log(data.length); // 1GBのデータがメモリに
});

// ✅ Stream：少しずつ処理
const readStream = fs.createReadStream('huge-file.txt');
readStream.on('data', (chunk) => {
  console.log(`受信したチャンク: ${chunk.length}bytes`);
  // メモリ使用量は常に一定
});
```

## Readable Stream

### 基本的な使い方

```javascript
const fs = require('fs');
const { pipeline } = require('stream/promises');

// ファイルからの読み取り
const readStream = fs.createReadStream('input.txt', {
  encoding: 'utf8',
  highWaterMark: 16 * 1024 // 16KBずつ読み取り
});

readStream.on('data', (chunk) => {
  console.log('データを受信:', chunk.length);
});

readStream.on('end', () => {
  console.log('読み取り完了');
});

readStream.on('error', (err) => {
  console.error('エラー:', err);
});
```

### カスタムReadable Streamの作成

```javascript
const { Readable } = require('stream');

class NumberStream extends Readable {
  constructor(options) {
    super(options);
    this.current = 0;
    this.max = options.max || 100;
  }

  _read() {
    if (this.current < this.max) {
      this.push(`数値: ${this.current}\n`);
      this.current++;
    } else {
      // null をpushするとストリーム終了
      this.push(null);
    }
  }
}

// 使用例
const numberStream = new NumberStream({ max: 10 });
numberStream.on('data', (chunk) => {
  console.log(chunk.toString().trim());
});
```

### 非同期イテレータとの組み合わせ

```javascript
const fs = require('fs');

async function processFile() {
  const readStream = fs.createReadStream('large-file.txt', { encoding: 'utf8' });
  
  try {
    for await (const chunk of readStream) {
      // 各チャンクを非同期で処理
      await processChunk(chunk);
    }
    console.log('処理完了');
  } catch (error) {
    console.error('処理エラー:', error);
  }
}

async function processChunk(chunk) {
  // 何らかの非同期処理
  return new Promise(resolve => {
    setTimeout(() => {
      console.log(`処理したチャンク: ${chunk.length}文字`);
      resolve();
    }, 100);
  });
}
```

## Writable Stream

### 基本的な使い方

```javascript
const fs = require('fs');

const writeStream = fs.createWriteStream('output.txt', {
  encoding: 'utf8'
});

// データの書き込み
writeStream.write('最初の行\n');
writeStream.write('2番目の行\n');
writeStream.write('3番目の行\n');

// ストリームを終了
writeStream.end('最後の行\n');

writeStream.on('finish', () => {
  console.log('書き込み完了');
});

writeStream.on('error', (err) => {
  console.error('書き込みエラー:', err);
});
```

### カスタムWritable Streamの作成

```javascript
const { Writable } = require('stream');

class UpperCaseWriteStream extends Writable {
  constructor(options) {
    super(options);
  }

  _write(chunk, encoding, callback) {
    // データを大文字に変換して出力
    const upperChunk = chunk.toString().toUpperCase();
    process.stdout.write(upperChunk);
    callback();
  }
}

// 使用例
const upperWriter = new UpperCaseWriteStream();
upperWriter.write('hello world\n');
upperWriter.write('node.js streams\n');
upperWriter.end();
```

### バックプレッシャーの処理

```javascript
const fs = require('fs');

const writeStream = fs.createWriteStream('output.txt');

function writeMillionTimes(writer, data, encoding, callback) {
  let i = 1000000;
  
  function write() {
    let ok = true;
    while (i > 0 && ok) {
      i--;
      if (i === 0) {
        // 最後のwrite
        writer.write(data, encoding, callback);
      } else {
        ok = writer.write(data, encoding);
      }
    }
    
    if (i > 0) {
      // バックプレッシャーが発生、drainイベントを待つ
      writer.once('drain', write);
    }
  }
  
  write();
}

writeMillionTimes(writeStream, 'データ\n', 'utf8', () => {
  console.log('100万回の書き込み完了');
  writeStream.end();
});
```

## Transform Stream

### 基本的な変換

```javascript
const { Transform } = require('stream');

class ReverseTransform extends Transform {
  _transform(chunk, encoding, callback) {
    // 各行を逆順にする
    const reversed = chunk.toString()
      .split('\n')
      .map(line => line.split('').reverse().join(''))
      .join('\n');
    
    callback(null, reversed);
  }
}

// 使用例
const fs = require('fs');
const reverseTransform = new ReverseTransform();

fs.createReadStream('input.txt')
  .pipe(reverseTransform)
  .pipe(fs.createWriteStream('output.txt'));
```

### JSON処理のためのTransform

```javascript
const { Transform } = require('stream');

class JSONLineTransform extends Transform {
  constructor(options) {
    super({ objectMode: true, ...options });
  }

  _transform(chunk, encoding, callback) {
    const lines = chunk.toString().split('\n');
    
    for (const line of lines) {
      if (line.trim()) {
        try {
          const jsonObj = JSON.parse(line);
          this.push(jsonObj);
        } catch (err) {
          this.emit('error', new Error(`無効なJSON: ${line}`));
          return;
        }
      }
    }
    
    callback();
  }
}

// 使用例
const jsonTransform = new JSONLineTransform();

jsonTransform.on('data', (obj) => {
  console.log('パースされたオブジェクト:', obj);
});

jsonTransform.write('{"name": "Alice", "age": 30}\n');
jsonTransform.write('{"name": "Bob", "age": 25}\n');
jsonTransform.end();
```

### CSVパーサーの実装

```javascript
const { Transform } = require('stream');

class CSVTransform extends Transform {
  constructor(options = {}) {
    super({ objectMode: true });
    this.headers = options.headers || null;
    this.separator = options.separator || ',';
    this.buffer = '';
  }

  _transform(chunk, encoding, callback) {
    this.buffer += chunk.toString();
    const lines = this.buffer.split('\n');
    
    // 最後の（不完全かもしれない）行をバッファに戻す
    this.buffer = lines.pop();

    for (const line of lines) {
      if (line.trim()) {
        const values = line.split(this.separator);
        
        if (!this.headers) {
          this.headers = values;
        } else {
          const obj = {};
          this.headers.forEach((header, index) => {
            obj[header.trim()] = values[index]?.trim() || '';
          });
          this.push(obj);
        }
      }
    }
    
    callback();
  }

  _flush(callback) {
    if (this.buffer.trim() && this.headers) {
      const values = this.buffer.split(this.separator);
      const obj = {};
      this.headers.forEach((header, index) => {
        obj[header.trim()] = values[index]?.trim() || '';
      });
      this.push(obj);
    }
    callback();
  }
}

// 使用例
const csvTransform = new CSVTransform();

csvTransform.on('data', (row) => {
  console.log('行データ:', row);
});

csvTransform.write('name,age,city\n');
csvTransform.write('Alice,30,Tokyo\n');
csvTransform.write('Bob,25,Osaka\n');
csvTransform.end();
```

## Pipeline と Composition

### pipeline() 関数の使用

```javascript
const { pipeline } = require('stream/promises');
const fs = require('fs');
const zlib = require('zlib');

async function compressFile() {
  try {
    await pipeline(
      fs.createReadStream('input.txt'),
      zlib.createGzip(),
      fs.createWriteStream('output.txt.gz')
    );
    console.log('圧縮完了');
  } catch (error) {
    console.error('パイプラインエラー:', error);
  }
}

compressFile();
```

### 複数のTransformを組み合わせ

```javascript
const { pipeline } = require('stream/promises');
const { Transform } = require('stream');
const fs = require('fs');

// 行番号を追加するTransform
class LineNumberTransform extends Transform {
  constructor() {
    super();
    this.lineNumber = 1;
  }

  _transform(chunk, encoding, callback) {
    const lines = chunk.toString().split('\n');
    const numberedLines = lines.map((line, index) => {
      if (index === lines.length - 1 && line === '') return line;
      return `${this.lineNumber++}: ${line}`;
    });
    callback(null, numberedLines.join('\n'));
  }
}

// 大文字変換Transform
class UpperCaseTransform extends Transform {
  _transform(chunk, encoding, callback) {
    callback(null, chunk.toString().toUpperCase());
  }
}

async function processFile() {
  try {
    await pipeline(
      fs.createReadStream('input.txt'),
      new LineNumberTransform(),
      new UpperCaseTransform(),
      fs.createWriteStream('output.txt')
    );
    console.log('ファイル処理完了');
  } catch (error) {
    console.error('処理エラー:', error);
  }
}
```

## HTTP Streamingの実践

### サーバーサイドでのStreaming

```javascript
const http = require('http');
const fs = require('fs');
const { pipeline } = require('stream/promises');

const server = http.createServer(async (req, res) => {
  if (req.method === 'GET' && req.url === '/download') {
    // 大容量ファイルのストリーミング配信
    res.setHeader('Content-Type', 'application/octet-stream');
    res.setHeader('Content-Disposition', 'attachment; filename="large-file.txt"');
    
    try {
      await pipeline(
        fs.createReadStream('large-file.txt'),
        res
      );
    } catch (error) {
      console.error('ストリーミングエラー:', error);
      if (!res.headersSent) {
        res.statusCode = 500;
        res.end('Internal Server Error');
      }
    }
  } else if (req.method === 'POST' && req.url === '/upload') {
    // ファイルアップロードの受信
    try {
      await pipeline(
        req,
        fs.createWriteStream('uploaded-file.txt')
      );
      res.end('Upload complete');
    } catch (error) {
      console.error('アップロードエラー:', error);
      res.statusCode = 500;
      res.end('Upload failed');
    }
  } else {
    res.statusCode = 404;
    res.end('Not Found');
  }
});

server.listen(3000, () => {
  console.log('サーバーが起動しました: http://localhost:3000');
});
```

### リアルタイムデータ配信

```javascript
const http = require('http');
const { Readable } = require('stream');

class RealTimeDataStream extends Readable {
  constructor(options) {
    super(options);
    this.interval = setInterval(() => {
      const data = {
        timestamp: new Date().toISOString(),
        value: Math.random() * 100,
        id: Math.random().toString(36).substr(2, 9)
      };
      
      this.push(`data: ${JSON.stringify(data)}\n\n`);
    }, 1000);
  }

  _read() {
    // 何もしない（setIntervalでデータをpush）
  }

  _destroy() {
    clearInterval(this.interval);
  }
}

const server = http.createServer((req, res) => {
  if (req.url === '/stream') {
    res.writeHead(200, {
      'Content-Type': 'text/plain',
      'Cache-Control': 'no-cache',
      'Connection': 'keep-alive',
      'Access-Control-Allow-Origin': '*'
    });

    const dataStream = new RealTimeDataStream();
    dataStream.pipe(res);

    req.on('close', () => {
      dataStream.destroy();
    });
  } else {
    res.statusCode = 404;
    res.end('Not Found');
  }
});

server.listen(3000);
```

## パフォーマンス最適化

### バッファサイズの調整

```javascript
const fs = require('fs');

// 小さなファイル：小さなバッファ
const smallFileStream = fs.createReadStream('small.txt', {
  highWaterMark: 4 * 1024 // 4KB
});

// 大きなファイル：大きなバッファ
const largeFileStream = fs.createReadStream('large.txt', {
  highWaterMark: 64 * 1024 // 64KB
});

// バッファサイズの測定
function measureThroughput(stream, name) {
  let totalBytes = 0;
  let chunkCount = 0;
  const startTime = Date.now();

  stream.on('data', (chunk) => {
    totalBytes += chunk.length;
    chunkCount++;
  });

  stream.on('end', () => {
    const duration = Date.now() - startTime;
    console.log(`${name}:`);
    console.log(`  総サイズ: ${totalBytes} bytes`);
    console.log(`  チャンク数: ${chunkCount}`);
    console.log(`  処理時間: ${duration} ms`);
    console.log(`  スループット: ${(totalBytes / duration * 1000 / 1024 / 1024).toFixed(2)} MB/s`);
  });
}
```

### 並列処理の実装

```javascript
const { Worker } = require('worker_threads');
const { Transform } = require('stream');

class ParallelTransform extends Transform {
  constructor(workerScript, options = {}) {
    super({ objectMode: true });
    this.workerScript = workerScript;
    this.concurrency = options.concurrency || 4;
    this.workers = [];
    this.queue = [];
    this.processing = 0;
    
    // ワーカーの初期化
    for (let i = 0; i < this.concurrency; i++) {
      this.createWorker();
    }
  }

  createWorker() {
    const worker = new Worker(this.workerScript);
    worker.available = true;
    this.workers.push(worker);
    
    worker.on('message', (result) => {
      worker.available = true;
      this.processing--;
      this.push(result);
      this.processQueue();
    });
  }

  _transform(chunk, encoding, callback) {
    this.queue.push({ chunk, callback });
    this.processQueue();
  }

  processQueue() {
    while (this.queue.length > 0 && this.processing < this.concurrency) {
      const availableWorker = this.workers.find(w => w.available);
      if (!availableWorker) break;

      const { chunk, callback } = this.queue.shift();
      availableWorker.available = false;
      this.processing++;
      availableWorker.postMessage(chunk);
      callback();
    }
  }

  _flush(callback) {
    // すべての処理が完了するまで待機
    const checkComplete = () => {
      if (this.processing === 0 && this.queue.length === 0) {
        this.workers.forEach(worker => worker.terminate());
        callback();
      } else {
        setTimeout(checkComplete, 10);
      }
    };
    checkComplete();
  }
}
```

## エラーハンドリングとデバッグ

### エラー処理のベストプラクティス

```javascript
const { pipeline } = require('stream/promises');
const { Transform } = require('stream');

class SafeTransform extends Transform {
  _transform(chunk, encoding, callback) {
    try {
      // 危険な処理
      const result = this.processDangerousData(chunk);
      callback(null, result);
    } catch (error) {
      // エラーを適切に処理
      this.emit('error', error);
    }
  }

  processDangerousData(chunk) {
    // 何らかの危険な処理
    const data = JSON.parse(chunk.toString());
    return JSON.stringify(data.result);
  }
}

async function robustPipeline() {
  try {
    await pipeline(
      sourceStream,
      new SafeTransform(),
      destinationStream
    );
  } catch (error) {
    console.error('パイプラインでエラーが発生:', error);
    // クリーンアップ処理
    await cleanup();
  }
}
```

### ストリームの監視

```javascript
function monitorStream(stream, name) {
  let bytesProcessed = 0;
  let chunksProcessed = 0;

  stream.on('data', (chunk) => {
    bytesProcessed += chunk.length;
    chunksProcessed++;
    
    if (chunksProcessed % 100 === 0) {
      console.log(`${name}: ${chunksProcessed} chunks, ${bytesProcessed} bytes`);
    }
  });

  stream.on('end', () => {
    console.log(`${name} 完了: ${chunksProcessed} chunks, ${bytesProcessed} bytes`);
  });

  stream.on('error', (err) => {
    console.error(`${name} エラー:`, err);
  });

  return stream;
}

// 使用例
const monitoredStream = monitorStream(fs.createReadStream('file.txt'), 'FileReader');
```

## 終わりに

Node.js Streamを使うことで：

1. **メモリ効率**: 大容量データも少ないメモリで処理
2. **リアルタイム処理**: データが到着次第すぐに処理開始
3. **組み合わせ可能**: パイプラインで複数の処理を連結
4. **スケーラビリティ**: バックプレッシャーによる自動的な流量制御

Streamは最初は複雑に感じるかもしれませんが、大容量データやリアルタイム処理には欠かせない技術です。
少しずつ慣れていって、効率的なNode.jsアプリケーションを構築していきましょう！ 
